{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim and Objective\n",
    "\n",
    "The aim of this project is to develop an AI model to optimize energy consumption in industrial plants. By anticipating and regulating energy demand in real-time, the model aims to reduce costs and increase efficiency. Specifically, the model will predict heating and cooling loads based on building characteristics and environmental conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "We are using the Energy Efficiency dataset from the UCI Machine Learning Repository. The dataset contains 768 samples and 8 features, along with two target variables: Heating Load and Cooling Load. The data is structured in a single CSV file with the following features:\n",
    "\n",
    "1. **Relative Compactness**: The ratio of the volume of the building to its external surface area.\n",
    "2. **Surface Area**: The total exterior surface area of the building.\n",
    "3. **Wall Area**: The total area of the walls in the building.\n",
    "4. **Roof Area**: The total area of the roof.\n",
    "5. **Overall Height**: The height of the building from the ground to the highest point.\n",
    "6. **Orientation**: The main orientation of the building (1-4 representing north, east, south, and west).\n",
    "7. **Glazing Area**: The total area of windows as a percentage of the exterior surface area.\n",
    "8. **Glazing Area Distribution**: The distribution of the glazing (0 means no glazing, 1-5 represent different configurations of glazing).\n",
    "\n",
    "The target variables are:\n",
    "- **Heating Load**: The energy required to maintain the indoor temperature at a comfortable level during cold periods.\n",
    "- **Cooling Load**: The energy required to maintain the indoor temperature at a comfortable level during hot periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'D:/phd docs/Mystic Minds/Energy efficiency/energy+efficiency/ENB2012_data.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X1      768 non-null    float64\n",
      " 1   X2      768 non-null    float64\n",
      " 2   X3      768 non-null    float64\n",
      " 3   X4      768 non-null    float64\n",
      " 4   X5      768 non-null    float64\n",
      " 5   X6      768 non-null    int64  \n",
      " 6   X7      768 non-null    float64\n",
      " 7   X8      768 non-null    int64  \n",
      " 8   Y1      768 non-null    float64\n",
      " 9   Y2      768 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 60.1 KB\n",
      "None\n",
      "               X1          X2          X3          X4         X5          X6  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
      "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
      "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
      "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
      "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
      "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
      "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
      "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
      "\n",
      "               X7         X8          Y1          Y2  \n",
      "count  768.000000  768.00000  768.000000  768.000000  \n",
      "mean     0.234375    2.81250   22.307195   24.587760  \n",
      "std      0.133221    1.55096   10.090204    9.513306  \n",
      "min      0.000000    0.00000    6.010000   10.900000  \n",
      "25%      0.100000    1.75000   12.992500   15.620000  \n",
      "50%      0.250000    3.00000   18.950000   22.080000  \n",
      "75%      0.400000    4.00000   31.667500   33.132500  \n",
      "max      0.400000    5.00000   43.100000   48.030000  \n",
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "X7    0\n",
      "X8    0\n",
      "Y1    0\n",
      "Y2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Significance of Features\n",
    "\n",
    "1. **Relative Compactness**: This feature helps to understand the efficiency of the building's design in terms of energy consumption. A more compact building generally requires less energy for heating and cooling.\n",
    "\n",
    "2. **Surface Area**: Larger surface areas can lead to higher energy loss or gain, affecting both heating and cooling requirements.\n",
    "\n",
    "3. **Wall Area**: The area of the walls contributes to heat transfer between the inside and outside of the building, impacting energy consumption.\n",
    "\n",
    "4. **Roof Area**: Similar to wall area, the roof's surface area affects the amount of heat transferred into or out of the building.\n",
    "\n",
    "5. **Overall Height**: The height can influence the stratification of air within the building, affecting heating and cooling efficiency.\n",
    "\n",
    "6. **Orientation**: The direction in which the building faces can influence solar gain, with some orientations receiving more sunlight than others, affecting the heating and cooling loads.\n",
    "\n",
    "7. **Glazing Area**: The amount of glazing (windows) can significantly affect heat loss or gain, impacting both heating and cooling needs.\n",
    "\n",
    "8. **Glazing Area Distribution**: The placement and distribution of glazing can influence the building's thermal performance, with certain configurations being more efficient than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features (X) and targets (y)\n",
    "X = data.iloc[:, :-2]  \n",
    "y_heating = data.iloc[:, -2]  # Second last column is Heating Load\n",
    "y_cooling = data.iloc[:, -1]  # Last column is Cooling Load\n",
    "\n",
    "X_train, X_test, y_train_heating, y_test_heating = train_test_split(X, y_heating, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_cooling, y_test_cooling = train_test_split(X, y_cooling, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression for Heating Load - MSE: 9.213843234012048, R^2: 0.9116028949393403\n",
      "Lasso Regression for Cooling Load - MSE: 13.752392534129699, R^2: 0.8515777827979275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "ridge.fit(X_train, y_train_heating)\n",
    "y_pred_heating = ridge.predict(X_test)\n",
    "mse_heating = mean_squared_error(y_test_heating, y_pred_heating)\n",
    "r2_heating = r2_score(y_test_heating, y_pred_heating)\n",
    "print(f'Ridge Regression for Heating Load - MSE: {mse_heating}, R^2: {r2_heating}')\n",
    "\n",
    "lasso.fit(X_train, y_train_cooling)\n",
    "y_pred_cooling = lasso.predict(X_test)\n",
    "mse_cooling = mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "r2_cooling = r2_score(y_test_cooling, y_pred_cooling)\n",
    "print(f'Lasso Regression for Cooling Load - MSE: {mse_cooling}, R^2: {r2_cooling}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "The training procedure involves several key steps:\n",
    "\n",
    "1. **Data Preprocessing**: \n",
    "   - **Normalization**: All numerical features are normalized to ensure that the model training process is not biased towards features with larger ranges.\n",
    "   - **Train-Test Split**: The dataset is divided into training and test sets to evaluate the model's performance. Typically, 70-80% of the data is used for training, and the remaining 20-30% is used for testing.\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - We use a variety of machine learning models to predict the Heating and Cooling Loads:\n",
    "     - **Linear Regression**: A basic linear model for initial benchmarking.\n",
    "     - **Ridge Regression**: A regularized linear model that penalizes large coefficients to prevent overfitting.\n",
    "     - **Lasso Regression**: Another regularized linear model that can also perform feature selection by setting some coefficients to zero.\n",
    "     - **Random Forest Regressor**: An ensemble method that uses multiple decision trees to improve predictive accuracy and control overfitting.\n",
    "     - **Gradient Boosting Regressor**: An ensemble technique that builds trees sequentially, each time trying to correct the errors of the previous tree.\n",
    "     - **Support Vector Regressor (SVR)**: A model that uses support vector machines for regression tasks.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - The models are evaluated using metrics such as Mean Squared Error (MSE) and R-squared (RÂ²) on both the training and test datasets.\n",
    "   - These metrics help in understanding how well the models have learned the patterns in the training data and how they generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heating Load Model Evaluation:\n",
      "Ridge - MSE: 9.213843234012048, R^2: 0.9116028949393403\n",
      "Lasso - MSE: 12.424268666600273, R^2: 0.8808022499700053\n",
      "Random Forest - MSE: 0.2414368229220765, R^2: 0.9976836684042364\n",
      "Gradient Boosting - MSE: 0.26489362841538155, R^2: 0.9974586251028782\n",
      "SVR - MSE: 7.9698407537153475, R^2: 0.9235377862928831\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cooling Load Model Evaluation:\n",
      "Ridge - MSE: 9.93717548762786, R^2: 0.8927533798254\n",
      "Lasso - MSE: 13.752392534129699, R^2: 0.8515777827979275\n",
      "Random Forest - MSE: 3.1477748566883004, R^2: 0.9660277495480768\n",
      "Gradient Boosting - MSE: 2.293051691795314, R^2: 0.9752523195210875\n",
      "SVR - MSE: 10.624078668708774, R^2: 0.8853400011797448\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "file_path = 'D:/phd docs/Mystic Minds/Energy efficiency/energy+efficiency/ENB2012_data.xlsx'\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-2]  \n",
    "y_heating = data.iloc[:, -2]  \n",
    "y_cooling = data.iloc[:, -1]  \n",
    "\n",
    "X_train, X_test, y_train_heating, y_test_heating = train_test_split(X, y_heating, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_cooling, y_test_cooling = train_test_split(X, y_cooling, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "print(\"Heating Load Model Evaluation:\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train_heating)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_heating, y_pred)\n",
    "    r2 = r2_score(y_test_heating, y_pred)\n",
    "    print(f\"{name} - MSE: {mse}, R^2: {r2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\") #==============================\n",
    "\n",
    "print(\"Cooling Load Model Evaluation:\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train_cooling)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_cooling, y_pred)\n",
    "    r2 = r2_score(y_test_cooling, y_pred)\n",
    "    print(f\"{name} - MSE: {mse}, R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter optimization is a crucial step to enhance the model's performance by fine-tuning the parameters that govern the learning process. For this, we use the following approaches:\n",
    "\n",
    "1. **Randomized Search with Cross-Validation**:\n",
    "   - We perform a randomized search over a predefined grid of hyperparameters for each model. This method samples a fixed number of parameter settings from the specified distributions, allowing a more efficient search than a complete grid search.\n",
    "   - Cross-validation (typically 3-fold) is used to evaluate each combination of parameters. This helps ensure that the model's performance is consistent across different subsets of the data.\n",
    "\n",
    "2. **Hyperparameters Tuning**:\n",
    "   - For **Ridge and Lasso Regression**, we tune the regularization parameter `alpha` to balance the trade-off between fitting the training data and maintaining a model with small weights.\n",
    "   - For **Random Forest**, parameters like the number of trees (`n_estimators`), maximum depth of the trees (`max_depth`), and the maximum number of features considered for splitting (`max_features`) are optimized.\n",
    "   - For **Gradient Boosting**, the number of boosting stages (`n_estimators`), the learning rate, and the maximum depth of the individual trees are tuned.\n",
    "   - For **SVR**, we optimize the regularization parameter `C`, kernel type, and kernel coefficient `gamma`.\n",
    "\n",
    "3. **Results**:\n",
    "   - The best set of hyperparameters is selected based on the cross-validation score, and the final model is trained on the entire training dataset.\n",
    "   - The optimized model is then evaluated on the test set to determine its generalization performance.\n",
    "   - This process helps in obtaining the best-performing model that balances the complexity and predictive power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Optimization Results:\n",
      "Optimizing Ridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 10 is smaller than n_iter=20. Running 10 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge: {'alpha': 0.1}\n",
      "Optimized Ridge - MSE: 9.158918084644545, R^2: 0.9121298438005049\n",
      "\n",
      "==================================================\n",
      "\n",
      "Optimizing Lasso...\n",
      "Best parameters for Lasso: {'alpha': 0.001}\n",
      "Optimized Lasso - MSE: 9.15818305552932, R^2: 0.9121368956293966\n",
      "\n",
      "==================================================\n",
      "\n",
      "Optimizing Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 10 is smaller than n_iter=20. Running 10 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "18 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "13 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [-0.76017361         nan -0.64704601         nan -0.97007823         nan\n",
      " -0.415388   -0.41106505         nan -0.62390764 -0.41329489 -0.61570118\n",
      " -0.45006328 -0.4239094  -0.5948864  -0.43943951 -0.62311038         nan\n",
      " -0.44026108         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 100, 'max_features': 'log2', 'max_depth': 20}\n",
      "Optimized Random Forest - MSE: 0.30818034962662, R^2: 0.9970433346811227\n",
      "\n",
      "==================================================\n",
      "\n",
      "Optimizing Gradient Boosting...\n",
      "Best parameters for Gradient Boosting: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.3}\n",
      "Optimized Gradient Boosting - MSE: 0.2104631988392117, R^2: 0.9979808276495832\n",
      "\n",
      "==================================================\n",
      "\n",
      "Optimizing SVR...\n",
      "Best parameters for SVR: {'kernel': 'rbf', 'gamma': 1.0, 'C': 100.0}\n",
      "Optimized SVR - MSE: 1.7119504985686453, R^2: 0.983575641104681\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'Ridge': {\n",
    "        'alpha': np.logspace(-3, 3, 10)\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-3, 3, 10)\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': np.logspace(-2, 2, 5),\n",
    "        'gamma': np.logspace(-2, 2, 5),\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "# Randomized Search with Cross-Validation for each model\n",
    "best_models = {}\n",
    "print(\"Hyperparameter Optimization Results:\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_distributions[name],\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        cv=n_folds,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train_heating)\n",
    "    best_models[name] = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    print(f\"Best parameters for {name}: {best_params}\")\n",
    "\n",
    "    y_pred = best_models[name].predict(X_test)\n",
    "    mse = mean_squared_error(y_test_heating, y_pred)\n",
    "    r2 = r2_score(y_test_heating, y_pred)\n",
    "    print(f\"Optimized {name} - MSE: {mse}, R^2: {r2}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
